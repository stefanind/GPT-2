{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbfdbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"A language model does\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e83d4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse and visualize the logfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sz = \"124M\"\n",
    "\n",
    "loss_baseline = {\n",
    "    \"124M\": 3.2924,\n",
    "}[sz]\n",
    "hella2_baseline = { # HellaSwag for GPT-2\n",
    "    \"124M\": 0.294463,\n",
    "    \"350M\": 0.375224,\n",
    "    \"774M\": 0.431986,\n",
    "    \"1558M\": 0.488946,\n",
    "}[sz]\n",
    "hella3_baseline = { # HellaSwag for GPT-3\n",
    "    \"124M\": 0.337,\n",
    "    \"350M\": 0.436,\n",
    "    \"774M\": 0.510,\n",
    "    \"1558M\": 0.547,\n",
    "}[sz]\n",
    "\n",
    "# load the log file\n",
    "with open(\"log/log.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# parse the individual lines, group by stream (train,val,hella)\n",
    "streams = {}\n",
    "for line in lines:\n",
    "    step, stream, val = line.strip().split()\n",
    "    if stream not in streams:\n",
    "        streams[stream] = {}\n",
    "    streams[stream][int(step)] = float(val)\n",
    "\n",
    "# convert each stream from {step: val} to (steps[], vals[])\n",
    "# so it's easier for plotting\n",
    "streams_xy = {}\n",
    "for k, v in streams.items():\n",
    "    # get all (step, val) items, sort them\n",
    "    xy = sorted(list(v.items()))\n",
    "    # unpack the list of tuples to tuple of lists\n",
    "    streams_xy[k] = list(zip(*xy))\n",
    "\n",
    "# create figure\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Panel 1: losses: both train and val\n",
    "plt.subplot(121)\n",
    "xs, ys = streams_xy[\"train\"] # training loss\n",
    "ys = np.array(ys)\n",
    "plt.plot(xs, ys, label=f'nanogpt ({sz}) train loss')\n",
    "print(\"Min Train Loss:\", min(ys))\n",
    "xs, ys = streams_xy[\"val\"] # validation loss\n",
    "plt.plot(xs, ys, label=f'nanogpt ({sz}) val loss')\n",
    "# horizontal line at GPT-2 baseline\n",
    "if loss_baseline is not None:\n",
    "    plt.axhline(y=loss_baseline, color='r', linestyle='--', label=f\"OpenAI GPT-2 ({sz}) checkpoint val loss\")\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.yscale('log')\n",
    "plt.ylim(top=4.0)\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "print(\"Min Validation Loss:\", min(ys))\n",
    "\n",
    "# Panel 2: HellaSwag eval\n",
    "plt.subplot(122)\n",
    "xs, ys = streams_xy[\"hella\"] # HellaSwag eval\n",
    "ys = np.array(ys)\n",
    "plt.plot(xs, ys, label=f\"nanogpt ({sz})\")\n",
    "# horizontal line at GPT-2 baseline\n",
    "if hella2_baseline:\n",
    "    plt.axhline(y=hella2_baseline, color='r', linestyle='--', label=f\"OpenAI GPT-2 ({sz}) checkpoint\")\n",
    "if hella3_baseline:\n",
    "    plt.axhline(y=hella3_baseline, color='g', linestyle='--', label=f\"OpenAI GPT-3 ({sz}) checkpoint\")\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"HellaSwag eval\")\n",
    "print(\"Max Hellaswag eval:\", max(ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144b2f08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca37ff16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39c8472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bc1640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f1ccd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import GPTConfig, GPT\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = GPT(GPTConfig(vocab_size=50304))\n",
    "ckpt = torch.load(\"log/model_19072.pt\", map_location=\"cpu\", weights_only=False)\n",
    "state = ckpt[\"model\"] if isinstance(ckpt, dict) and \"model\" in ckpt else ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d43872fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: A language model does not have a clear definition of what it means to be a language learner.\n",
      "The term “\n"
     ]
    }
   ],
   "source": [
    "# greedy sampling\n",
    "\n",
    "\n",
    "model.load_state_dict(state)\n",
    "model.to(device)\n",
    "enc = tiktoken.get_encoding(\"gpt2\") \n",
    "\n",
    "model.eval()\n",
    "num_return_sequences = 1\n",
    "max_length = 25\n",
    "tokens = enc.encode(\"A language model does\")\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1) # convert to a sample batch\n",
    "xgen = tokens.to(\"cpu\")\n",
    "while xgen.size(1) < max_length:\n",
    "    with torch.no_grad():\n",
    "        logits, loss = model(xgen) # (B, T, vocab_size)\n",
    "        logits = logits[:, -1, :] # (B, vocab_size)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        xcol = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "        \n",
    "        xgen = torch.cat((xgen, xcol), dim=1)\n",
    "for i in range(num_return_sequences):\n",
    "    tokens = xgen[i, :max_length].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(f\"{i}: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2d9310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c18848a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: A language model does not allow for a system to learn. This principle is illustrated in the following illustration from the previous page:\n",
      "The following example demonstrates the use of the language model.\n",
      "You can view and modify the language model, or you can\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# top k sampling\n",
    "\n",
    "model.load_state_dict(state)\n",
    "model.to(device)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "model.eval()\n",
    "num_return_sequences = 1\n",
    "max_length = 50\n",
    "tokens = enc.encode(\"A language model does\")\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1) # convert to a sample batch\n",
    "xgen = tokens.to(\"cpu\")\n",
    "while xgen.size(1) < max_length:\n",
    "    with torch.no_grad():\n",
    "        logits, loss = model(xgen) # (B, T, vocab_size)\n",
    "        logits = logits[:, -1, :] # (B, vocab_size)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        topk_probs, topk_idxs = torch.topk(probs, 50, dim=-1)\n",
    "        ix = torch.multinomial(topk_probs, 1)\n",
    "        xcol = torch.gather(topk_idxs, dim=-1, index=ix)\n",
    "        xgen = torch.cat((xgen, xcol), dim=1)\n",
    "\n",
    "for i in range(num_return_sequences):\n",
    "    tokens = xgen[i, :max_length].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(f\"{i}: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45a2a184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: A language model does not predict on how many families depend upon English. For example, the first step of learning to read is trying to learn how to use slang. A reading therapist is a person who has spent most of their careers learning and making oral\n"
     ]
    }
   ],
   "source": [
    "num_return_sequences = 1\n",
    "max_length = 50\n",
    "tokens = enc.encode(\"A language model does\")\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1) # convert to a sample batch\n",
    "xgen = tokens.to(\"cpu\")\n",
    "while xgen.size(1) < max_length:\n",
    "    with torch.no_grad():\n",
    "        logits, loss = model(xgen) # (B, T, vocab_size)\n",
    "        logits = logits[:, -1, :] # (B, vocab_size)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        sorted_probs, sorted_idxs = torch.sort(probs, descending=True, dim=-1)\n",
    "\n",
    "        cumprobs = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "        mask = cumprobs > 0.9\n",
    "        mask[:, 1:] = mask[:, :-1].clone()\n",
    "        mask[:, 0] = False\n",
    "\n",
    "        sorted_probs[mask] = 0.0\n",
    "        sorted_probs = sorted_probs / sorted_probs.sum(-1, keepdim=True)\n",
    "\n",
    "        ix = torch.multinomial(sorted_probs, 1) # (B, 1)\n",
    "        xcol = torch.gather(sorted_idxs, -1, ix) # (B, 1)\n",
    "        xgen = torch.cat((xgen, xcol), dim=1)\n",
    "for i in range(num_return_sequences):\n",
    "    tokens = xgen[i, :max_length].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(f\"{i}: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff47c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "64 * 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd506e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "301966 * 20 // 32768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ee8374",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gelu_tanh(x: torch.Tensor) -> torch.Tensor:\n",
    "    # Approximate GELU from the original GPT papers\n",
    "    sqrt_2_over_pi = math.sqrt(2.0 / math.pi)\n",
    "    return 0.5 * x * (\n",
    "        1.0 + torch.tanh(sqrt_2_over_pi * (x + 0.044715 * x * x * x))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e81f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda.nccl as nccl\n",
    "import torch.distributed as dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9504fa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a873e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_bfloat_support = (\n",
    "    torch.version.cuda\n",
    "    and torch.cuda.is_bf16_supported()\n",
    "    and packaging.version.parse(torch.version.cuda).release >= (11, 0)\n",
    "    and dist.is_nccl_available()\n",
    "    and nccl.version() >= (2, 10)\n",
    "    )\n",
    "\n",
    "# torch.cuda.is_bf16_supported() does not confirm network can handle it\n",
    "# just gpu native support\n",
    "# “The GPU can compute BF16, but NCCL might not be able to reduce BF16.”\n",
    "\n",
    "# basically means that when using FSDP, calling torch.cuda.is_bf16... only lets\n",
    "# us know if the GPU supports bf16, but it dn guarantee that the dist\n",
    "# communication stack can safely use it!\n",
    "# compute support != communication support\n",
    "# so the code says nothing about multi-GPU communication\n",
    "# the communication backend, i.e., NCCL's collective algorithms\n",
    "# the reduction and accum kernels!!\n",
    "# correct BF16 arithmetic kernels, accum semantics, packaging/unpackaging of bf16 data\n",
    "# GPU-GPU comms path, IMPLEMENTED BY NCCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae24aa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "1000000000000000000.0 == int(1000000000000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8255cb61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
