{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbfdbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"A language model does\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e83d4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse and visualize the logfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sz = \"124M\"\n",
    "\n",
    "loss_baseline = {\n",
    "    \"124M\": 3.2924,\n",
    "}[sz]\n",
    "hella2_baseline = { # HellaSwag for GPT-2\n",
    "    \"124M\": 0.294463,\n",
    "    \"350M\": 0.375224,\n",
    "    \"774M\": 0.431986,\n",
    "    \"1558M\": 0.488946,\n",
    "}[sz]\n",
    "hella3_baseline = { # HellaSwag for GPT-3\n",
    "    \"124M\": 0.337,\n",
    "    \"350M\": 0.436,\n",
    "    \"774M\": 0.510,\n",
    "    \"1558M\": 0.547,\n",
    "}[sz]\n",
    "\n",
    "# load the log file\n",
    "with open(\"log/log.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# parse the individual lines, group by stream (train,val,hella)\n",
    "streams = {}\n",
    "for line in lines:\n",
    "    step, stream, val = line.strip().split()\n",
    "    if stream not in streams:\n",
    "        streams[stream] = {}\n",
    "    streams[stream][int(step)] = float(val)\n",
    "\n",
    "# convert each stream from {step: val} to (steps[], vals[])\n",
    "# so it's easier for plotting\n",
    "streams_xy = {}\n",
    "for k, v in streams.items():\n",
    "    # get all (step, val) items, sort them\n",
    "    xy = sorted(list(v.items()))\n",
    "    # unpack the list of tuples to tuple of lists\n",
    "    streams_xy[k] = list(zip(*xy))\n",
    "\n",
    "# create figure\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Panel 1: losses: both train and val\n",
    "plt.subplot(121)\n",
    "xs, ys = streams_xy[\"train\"] # training loss\n",
    "ys = np.array(ys)\n",
    "plt.plot(xs, ys, label=f'nanogpt ({sz}) train loss')\n",
    "print(\"Min Train Loss:\", min(ys))\n",
    "xs, ys = streams_xy[\"val\"] # validation loss\n",
    "plt.plot(xs, ys, label=f'nanogpt ({sz}) val loss')\n",
    "# horizontal line at GPT-2 baseline\n",
    "if loss_baseline is not None:\n",
    "    plt.axhline(y=loss_baseline, color='r', linestyle='--', label=f\"OpenAI GPT-2 ({sz}) checkpoint val loss\")\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.yscale('log')\n",
    "plt.ylim(top=4.0)\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "print(\"Min Validation Loss:\", min(ys))\n",
    "\n",
    "# Panel 2: HellaSwag eval\n",
    "plt.subplot(122)\n",
    "xs, ys = streams_xy[\"hella\"] # HellaSwag eval\n",
    "ys = np.array(ys)\n",
    "plt.plot(xs, ys, label=f\"nanogpt ({sz})\")\n",
    "# horizontal line at GPT-2 baseline\n",
    "if hella2_baseline:\n",
    "    plt.axhline(y=hella2_baseline, color='r', linestyle='--', label=f\"OpenAI GPT-2 ({sz}) checkpoint\")\n",
    "if hella3_baseline:\n",
    "    plt.axhline(y=hella3_baseline, color='g', linestyle='--', label=f\"OpenAI GPT-3 ({sz}) checkpoint\")\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"HellaSwag eval\")\n",
    "print(\"Max Hellaswag eval:\", max(ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144b2f08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca37ff16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39c8472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bc1640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f1ccd7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import GPTConfig, GPT\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = GPT(GPTConfig(vocab_size=50304))\n",
    "ckpt = torch.load(\"log/model_19072.pt\", map_location=\"cpu\", weights_only=False)\n",
    "state = ckpt[\"model\"] if isinstance(ckpt, dict) and \"model\" in ckpt else ckpt\n",
    "\n",
    "model.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43872fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: A language model does not have a clear definition of what it means to be a language learner.\n",
      "The term “\n"
     ]
    }
   ],
   "source": [
    "# greedy sampling\n",
    "\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "enc = tiktoken.get_encoding(\"gpt2\") \n",
    "\n",
    "model.eval()\n",
    "num_return_sequences = 1\n",
    "max_length = 25\n",
    "tokens = enc.encode(\"A language model does\")\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1) # convert to a sample batch\n",
    "xgen = tokens.to(\"cpu\")\n",
    "while xgen.size(1) < max_length:\n",
    "    with torch.no_grad():\n",
    "        logits, loss, _ = model(xgen) # (B, T, vocab_size)\n",
    "        logits = logits[:, -1, :] # (B, vocab_size)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        xcol = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "        \n",
    "        xgen = torch.cat((xgen, xcol), dim=1)\n",
    "for i in range(num_return_sequences):\n",
    "    tokens = xgen[i, :max_length].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(f\"{i}: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2d9310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c18848a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: A language model does not allow for a system to learn. This principle is illustrated in the following illustration from the previous page:\n",
      "The following example demonstrates the use of the language model.\n",
      "You can view and modify the language model, or you can\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# top k sampling\n",
    "\n",
    "model.load_state_dict(state)\n",
    "model.to(device)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "model.eval()\n",
    "num_return_sequences = 1\n",
    "max_length = 50\n",
    "tokens = enc.encode(\"A language model does\")\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1) # convert to a sample batch\n",
    "xgen = tokens.to(\"cpu\")\n",
    "while xgen.size(1) < max_length:\n",
    "    with torch.no_grad():\n",
    "        logits, loss = model(xgen) # (B, T, vocab_size)\n",
    "        logits = logits[:, -1, :] # (B, vocab_size)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        topk_probs, topk_idxs = torch.topk(probs, 50, dim=-1)\n",
    "        ix = torch.multinomial(topk_probs, 1)\n",
    "        xcol = torch.gather(topk_idxs, dim=-1, index=ix)\n",
    "        xgen = torch.cat((xgen, xcol), dim=1)\n",
    "\n",
    "for i in range(num_return_sequences):\n",
    "    tokens = xgen[i, :max_length].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(f\"{i}: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45a2a184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: A language model does not predict on how many families depend upon English. For example, the first step of learning to read is trying to learn how to use slang. A reading therapist is a person who has spent most of their careers learning and making oral\n"
     ]
    }
   ],
   "source": [
    "num_return_sequences = 1\n",
    "max_length = 50\n",
    "tokens = enc.encode(\"A language model does\")\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1) # convert to a sample batch\n",
    "xgen = tokens.to(\"cpu\")\n",
    "while xgen.size(1) < max_length:\n",
    "    with torch.no_grad():\n",
    "        logits, loss = model(xgen) # (B, T, vocab_size)\n",
    "        logits = logits[:, -1, :] # (B, vocab_size)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        sorted_probs, sorted_idxs = torch.sort(probs, descending=True, dim=-1)\n",
    "\n",
    "        cumprobs = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "        mask = cumprobs > 0.9\n",
    "        mask[:, 1:] = mask[:, :-1].clone()\n",
    "        mask[:, 0] = False\n",
    "\n",
    "        sorted_probs[mask] = 0.0\n",
    "        sorted_probs = sorted_probs / sorted_probs.sum(-1, keepdim=True)\n",
    "\n",
    "        ix = torch.multinomial(sorted_probs, 1) # (B, 1)\n",
    "        xcol = torch.gather(sorted_idxs, -1, ix) # (B, 1)\n",
    "        xgen = torch.cat((xgen, xcol), dim=1)\n",
    "for i in range(num_return_sequences):\n",
    "    tokens = xgen[i, :max_length].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(f\"{i}: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a873e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda.nccl as nccl\n",
    "import torch.distributed as dist\n",
    "\n",
    "\n",
    "verify_bfloat_support = (\n",
    "    torch.version.cuda\n",
    "    and torch.cuda.is_bf16_supported()\n",
    "    and packaging.version.parse(torch.version.cuda).release >= (11, 0)\n",
    "    and dist.is_nccl_available()\n",
    "    and nccl.version() >= (2, 10)\n",
    "    )\n",
    "\n",
    "# torch.cuda.is_bf16_supported() does not confirm network can handle it\n",
    "# just gpu native support\n",
    "# “The GPU can compute BF16, but NCCL might not be able to reduce BF16.”\n",
    "\n",
    "# basically means that when using FSDP, calling torch.cuda.is_bf16... only lets\n",
    "# us know if the GPU supports bf16, but it dn guarantee that the dist\n",
    "# communication stack can safely use it!\n",
    "# compute support != communication support\n",
    "# so the code says nothing about multi-GPU communication\n",
    "# the communication backend, i.e., NCCL's collective algorithms\n",
    "# the reduction and accum kernels!!\n",
    "# correct BF16 arithmetic kernels, accum semantics, packaging/unpackaging of bf16 data\n",
    "# GPU-GPU comms path, IMPLEMENTED BY NCCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8255cb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_kv(model, idx, new_tokens):\n",
    "    logits, _, past_kv = model(idx, past_kv=None)\n",
    "    for _ in range(new_tokens):\n",
    "        next_id = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "        logits, _, past_kv = model(next_id, past_kv=past_kv)\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_no_kv(model, idx, new_tokens):\n",
    "    for _ in range(new_tokens):\n",
    "        logits, _, _ = model(idx, past_kv=None)\n",
    "        next_id = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "\n",
    "def torch_profile(model, device=\"cpu\", B=1, T=128, new_tokens=64):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    idx = torch.randint(0, model.config.vocab_size, (B, T), device=device)\n",
    "\n",
    "    for name, function in [(\"no_kv\", run_no_kv), (\"kv\", run_kv)]:\n",
    "        with profile(activities=[ProfilerActivity.CPU]) as profiler:\n",
    "            function(model, idx, new_tokens)\n",
    "        print(f\"\\n--- {name} ---\")\n",
    "        print(profiler.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "531a98e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- no_kv ---\n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                          aten::addmm        53.23%        5.979s        55.44%        6.227s       2.027ms          3072  \n",
      "                                             aten::mm        31.03%        3.486s        31.03%        3.486s      54.464ms            64  \n",
      "    aten::_scaled_dot_product_flash_attention_for_cpu         3.99%     448.060ms         4.49%     504.540ms     656.954us           768  \n",
      "                                           aten::gelu         3.73%     418.662ms         3.73%     418.662ms     545.133us           768  \n",
      "                                          aten::copy_         2.02%     226.808ms         2.02%     226.808ms      73.831us          3072  \n",
      "                                         aten::linear         0.90%     100.704ms        88.53%        9.944s       3.171ms          3136  \n",
      "                              aten::native_layer_norm         0.89%      99.584ms         1.08%     121.031ms      75.644us          1600  \n",
      "                                            aten::add         0.85%      95.900ms         0.85%      95.900ms      59.938us          1600  \n",
      "                                           aten::view         0.67%      75.472ms         0.67%      75.472ms       5.986us         12608  \n",
      "                                      aten::transpose         0.53%      59.425ms         0.70%      78.346ms       7.244us         10816  \n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 11.232s\n",
      "\n",
      "\n",
      "--- kv ---\n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                          aten::addmm        54.87%        1.441s        55.96%        1.470s     471.038us          3120  \n",
      "                                             aten::mm        26.64%     699.723ms        26.65%     699.789ms      10.766ms            65  \n",
      "                                            aten::cat         5.05%     132.698ms         5.81%     152.596ms      99.346us          1536  \n",
      "    aten::_scaled_dot_product_flash_attention_for_cpu         1.76%      46.212ms         2.67%      70.149ms      89.934us           780  \n",
      "                                         aten::linear         1.28%      33.668ms        85.81%        2.254s     707.585us          3185  \n",
      "                                      aten::transpose         1.11%      29.059ms         1.51%      39.753ms       3.619us         10985  \n",
      "                                           aten::view         0.93%      24.380ms         0.93%      24.380ms       1.904us         12805  \n",
      "                              aten::native_layer_norm         0.90%      23.616ms         1.26%      33.185ms      20.422us          1625  \n",
      "                                           aten::gelu         0.81%      21.387ms         0.81%      21.387ms      27.420us           780  \n",
      "                                     aten::as_strided         0.77%      20.241ms         0.77%      20.241ms       1.020us         19841  \n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.626s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch_profile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df29cfc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
